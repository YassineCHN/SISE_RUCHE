import os
import streamlit as st
from sympy import re
import duckdb
import pandas as pd
import plotly.express as px
from streamlit.config import MOTHERDUCK_DATABASE
from dotenv import load_dotenv

# --- CHARGEMENT DES VARIABLES D'ENVIRONNEMENT ---
dovenv_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.env')
load_dotenv(dovenv_path)
MOTHERDUCK_TOKEN = os.getenv("MOTHERDUCK_TOKEN")

st.set_page_config(layout="wide", page_title="Visualisation des donn√©es", page_icon="üíº")
st.sidebar.image("./static/Logo3.png", width=150)
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');
    html, body, [class*="css"] { font-family: 'Inter', sans-serif; }
    [data-testid="stSidebar"] { background: linear-gradient(180deg, #f8f9fa 0%, #e9ecef 100%); }
</style>
""", unsafe_allow_html=True)

# --- CONNEXION MOTHERDUCK ---
@st.cache_resource
def get_motherduck_connection():
    """Connexion √† MotherDuck (fail-fast, sans try/except)."""
    
    if not MOTHERDUCK_TOKEN:
        st.error("‚ùå Token MotherDuck manquant")
        st.stop()

    con = duckdb.connect(f"md:?motherduck_token={MOTHERDUCK_TOKEN}")
    con.execute(f"CREATE DATABASE IF NOT EXISTS {MOTHERDUCK_DATABASE}")
    con.close()

    return duckdb.connect(
        f"md:{MOTHERDUCK_DATABASE}?motherduck_token={MOTHERDUCK_TOKEN}"
    )

db = get_motherduck_connection()
# --- CHARGEMENT DES DONN√âES ---
@st.cache_data
def load_data():
    query = """
    SELECT 
        job_id, 
        title, 
        salaire, 
        nom_region, 
        hard_skills, 
        id_date_publication
    FROM f_offre f
    LEFT JOIN d_localisation l ON f.id_ville = l.id_ville
    LEFT JOIN h_region r ON l.id_region = r.id_region
    LEFT JOIN d_contrat c ON f.id_contrat = c.id_contrat
    LEFT JOIN d_date d ON f.id_date_publication = d.id_date
    """
    return db.execute(query).df()

# --- TRAITEMENT DES COMP√âTENCES (Top 5) ---
def get_top_skills(dataframe):
    # On explose la colonne comp√©tences si elle est stock√©e sous forme de liste ou texte
    all_skills = dataframe['hard_skills'].str.split(',').explode().str.strip()
    return all_skills.value_counts().head(5)

# --- FONCTION DE NETTOYAGE DES SALAIRES ---
def parse_salary_range(salary_str):
    if pd.isna(salary_str) or not isinstance(salary_str, str):
        return None
    
    # 1. Nettoyage de base : passage en minuscule et remplacement du 'k' et des espaces √©ventuels
    clean_str = salary_str.lower().replace(' ', '').replace('k', '000').replace('‚Ç¨', '')
    
    # Extraction de tous les nombres pr√©sents dans la cha√Æne
    numbers = re.findall(r'\d+', clean_str)
    if not numbers:
        return None
    
    # Convertir les extractions en float
    vals = [float(n) for n in numbers]

    # 2. Logique selon les op√©rateurs
    if '<' in clean_str:
        # Cas "< 25k‚Ç¨" -> On prend la borne haute
        return vals[0]
    
    elif '>' in clean_str:
        # Cas "> 100k‚Ç¨" -> On prend la borne basse
        return vals[0]
    
    elif '-' in clean_str or '√†' in clean_str:
        # Cas "25k - 30k" -> Moyenne de l'intervalle
        if len(vals) >= 2:
            return (vals[0] + vals[1]) / 2
        return vals[0]
    else:
        # Cas "40000" (valeur unique)
        return vals[0]
        
# --- FONCTION DE CLUSTERING DES TITRES---
def cluster_job_titles(df, column='title'):
    # 1. Nettoyage et extraction des titres uniques pour optimiser le calcul
    unique_titles = df[column].unique().tolist()
    
    # 2. Embedding (Encodage s√©mantique)
    # On utilise un mod√®le multilingue performant pour le fran√ßais
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    embeddings = model.encode(unique_titles, show_progress_bar=True)
    
    # 3. R√©duction de dimension avec UMAP
    # On r√©duit √† 5 dimensions pour aider HDBSCAN √† trouver les densit√©s
    reducer = umap.UMAP(
        n_neighbors=15, 
        n_components=5, 
        metric='cosine', 
        random_state=42
    )
    u_embeddings = reducer.fit_transform(embeddings)
    
    # 4. Clustering avec HDBSCAN
    # min_cluster_size : taille minimale pour former un groupe
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=3, 
        metric='euclidean', 
        cluster_selection_method='eom'
    )
    labels = clusterer.fit_labels(u_embeddings)
    
    # 5. Cr√©ation d'une table de correspondance
    mapping_df = pd.DataFrame({
        column: unique_titles,
        'cluster_id': labels
    })
    
    # Nommer les clusters par leur titre le plus fr√©quent
    def get_representative_name(cluster_id):
        if cluster_id == -1: return "Autres / Non class√©"
        # On prend le titre le plus court du cluster comme nom "propre"
        cluster_titles = mapping_df[mapping_df['cluster_id'] == cluster_id][column]
        return min(cluster_titles, key=len)

    cluster_names = {cid: get_representative_name(cid) for cid in set(labels)}
    mapping_df['titre_standardise'] = mapping_df['cluster_id'].map(cluster_names)
    
    # Fusionner avec le dataframe d'origine
    return df.merge(mapping_df[[column, 'titre_standardise']], on=column, how='left')
    
    # --- PR√âPARATION DES DONN√âES ---
@st.cache_data
df = load_data()
df['salaire_moyen'] = df['salaire'].apply(parse_salary_range)
# Suppression des lignes o√π le salaire n'a pas pu √™tre extrait
df_clean = df.dropna(subset=['salaire_moyen'])
#Cluster des titres
df_clean = cluster_job_titles(df_clean)

# --- INTERFACE STREAMLIT ---
st.divider()
st.sidebar.markdown("# Visualisation du March√© de l'Emploi Data et IA")

# Row 1: KPIs
col1, col2, col3 = st.columns(3)
with col1:
    st.metric("Offres analys√©es", len(df_clean))
with col2:
    global_avg = df_clean['salaire_moyen'].mean()
    st.metric("Salaire Moyen Global", f"{global_avg:,.0f} ‚Ç¨")
with col3:
    st.metric("Salaire M√©dian", f"{df_clean['salaire_moyen'].median():,.0f} ‚Ç¨")

st.divider()
# Row 2: Graphiques
col_left, col_right = st.columns([2, 1])

with col_left:
    st.subheader(" Salaire moyen par intitul√© de poste")
    # Groupement par intitul√© (on prend les 5 plus fr√©quents pour la lisibilit√©)
    top_jobs = df_clean['titre_standardise'].value_counts().nlargest(5).index
    df_top_jobs = df_clean[df_clean['titre_standardise'].isin(top_jobs)]
    
    avg_salary_cluster = df_salaires.groupby('titre_standardise')['salaire_moyen'].mean().sort_values(ascending=True).reset_index()

fig_salary = px.bar(
    avg_salary_cluster,
    x='salaire_moyen',
    y='titre_standardise',
    orientation='h',
    text_auto='.2s',
    labels={'salaire_moyen': 'Salaire Moyen (‚Ç¨)', 'titre_standardise': 'M√©tier (Cluster)'},
    color='salaire_moyen',
    color_continuous_scale='BuGn'
)
fig_salary.update_layout(yaxis={'categoryorder':'total ascending'})
st.plotly_chart(fig_salary, use_container_width=True)
    )
    st.plotly_chart(fig_salary, use_container_width=True)

with col_right:
    st.subheader(" Salaires par R√©gion")
    avg_salary_region = df_clean.groupby('region')['salaire_moyen'].mean().sort_values(ascending=False).reset_index()
    
    fig_region_sal = px.scatter(
        avg_salary_region,
        x='region',
        y='salaire_moyen',
        size='salaire_moyen',
        color='region',
        showlegend=False
    )
    st.plotly_chart(fig_region_sal, use_container_width=True)

# Row 3: Comp√©tences (Rappel du besoin pr√©c√©dent)
st.subheader(" Top 5 des comp√©tences les plus demand√©es")
all_skills = df['hard_skills'].str.split(',').explode().str.strip()
top_skills = all_skills.value_counts().head(5).reset_index()
st.table(top_skills.rename(columns={'count': 'Nombre d\'offres', 'hard_skills': 'Comp√©tence'}))

# 1. Pr√©paration des donn√©es : on garde le lien entre le titre et les comp√©tences
# S√©paration des comp√©tences en listes
df['skill_list'] = df['hard_skills'].str.split(',')

# "explode" sur la liste de comp√©tences (le titre est dupliqu√© pour chaque comp√©tence)
df_exploded = df.explode('skill_list')

# Nettoyage : espaces et mise en forme
df_exploded['skill_list'] = df_exploded['skill_list'].str.strip().str.capitalize()

# Suppression des lignes vides √©ventuelles
df_exploded = df_exploded[df_exploded['skill_list'] != ""]

# 2. Agr√©gation pour le graphique (on compte le nombre d'occurrences par m√©tier et comp√©tence)
top_7_skills = df_exploded['skill_list'].value_counts().nlargest(7).index
sunburst_data = (
    df_exploded[df_exploded['skill_list'].isin(top_7_skills)]
    .groupby(['title_standardis√©', 'skill_list'])
    .size()
    .reset_index(name='Nombre d\'offres')
)

# 3. Cr√©ation du graphique Sunburst
fig = px.sunburst(
    sunburst_data,
    path=['title_standardis√©', 'skill_list'],
    values='Nombre d\'offres',
    color='title_standardis√©',               
    title="Top comp√©tences par intitul√© de poste"
)

# Affichage dans Streamlit
st.plotly_chart(fig, use_container_width=True)

with st.container(border=False, horizontal=True, horizontal_alignment="center"):
    st.markdown(
        "<p style='text-align: center;'>Date de derni√®re Mise √† jour de la BDD : DD//MM//YYYY</p>",
        unsafe_allow_html=True,
    )

st.markdown("<br><br>", unsafe_allow_html=True)
st.markdown("<div style='text-align: center; color: #718096; font-size: 0.9rem;'>Powered by <strong>MotherDuck</strong> √ó <strong>Sentence Transformers</strong> | RUCHE Team ¬© 2026</div>", unsafe_allow_html=True)
