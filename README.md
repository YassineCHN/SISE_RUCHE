# ğŸ RUCHE
Projet NLP & Text Mining â€“ Master 2 SISE (2025â€“2026) 

### Application de cherche d'emplois

![Logo](streamlit/static/Logo3.png)

## PrÃ©sentation du projet

**RUCHE** est une plateforme dâ€™analyse du marchÃ© de lâ€™emploi **Data Science & Intelligence Artificielle** en France.  
Elle combine **web scraping**, **NLP**, **machine learning**, **data warehousing** et **visualisation interactive** pour proposer :

-  **Une recherche sÃ©mantique intelligente** dâ€™offres dâ€™emploi  
-  **Une cartographie gÃ©ographique interactive** du marchÃ© de lâ€™emploi  
-  **Des analyses avancÃ©es** sur les salaires, les compÃ©tences et les tendances du marchÃ©
-  **Enregistrement de nouvelles offres** pour les utilisateurs de l'application

Le systÃ¨me repose sur une **architecture end-to-end**, depuis la collecte des donnÃ©es jusquâ€™Ã  leur exploitation analytique au sein dâ€™une application **Streamlit**.

## Architecture

```
RUCHE/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ backup_job_market.duckdb
â”‚   â””â”€â”€ local.duckdb
â”‚
â”œâ”€â”€ scraping/
â”‚   â”œâ”€â”€ francetravail/
â”‚   â”œâ”€â”€ apec/
â”‚   â”œâ”€â”€ jobteaser/
â”‚   â””â”€â”€ service_public/
â”‚
â”œâ”€â”€ mongodb/
â”‚   â”œâ”€â”€ main_mongo.py
â”‚   â”œâ”€â”€ reference_apec.py
â”‚   â”œâ”€â”€ mongodb_load_jobteaser.py
â”‚   â””â”€â”€ mongodb_utils.py
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ cleanX.py #Tout les "clean" fpnction de nettoyage de donnÃ©e
â”‚   â”œâ”€â”€ config_etl.py
â”‚   â”œâ”€â”€ etl_utils.py
â”‚   â”œâ”€â”€ etl_vectorization.py
â”‚   â”œâ”€â”€ tfidf_ml_data_filter.py
â”‚   â”œâ”€â”€ geolocation_enrichment.py # API pour longÃ©tude et latitude 
â”‚   â””â”€â”€ etl_motherduck.py
â”‚
â”œâ”€â”€ streamlit_app/
â”‚   â”œâ”€â”€ 1_home_page.py
â”‚   â”œâ”€â”€ 2_cartographie.py
â”‚   â”œâ”€â”€ 3_visualisation.py
â”‚   â”œâ”€â”€ 4_add_offers.py
â”‚   â”œâ”€â”€ 5_clustering.py
â”‚   â”œâ”€â”€ 6_graphe_competences.py
â”‚   â”œâ”€â”€ 7_llm.py
â”‚   â”œâ”€â”€ 8_about.py
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ static/ # Logo & images
â”‚   â”œâ”€â”€ db/
â”‚   â””â”€â”€ analyse_competences/
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ Rapport.md
â”‚   â”œâ”€â”€ notice_france_travail_scraper.md
â”‚   â”œâ”€â”€ notice_TFIDF_ML_filtre_data_nondata.md
â”‚   â””â”€â”€ notice_moteur_recherche_semantique.md
â”‚
â”œâ”€â”€ duck_to_mother.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ test_connexion_duckdb.py
â”œâ”€â”€ test_creation_duckdb.py
â””â”€â”€ README.md

```

## Setup

1. **Install dependencies:**
```bash
pip install -r requirements_mongodo_ftscraper.txt
```

2. **Configure `.env` file:**
```env
MONGO_URI=mongodb+srv://username:password@cluster.mongodb.net/
FT_CLIENT_ID=your_ft_client_id
FT_CLIENT_SECRET=your_ft_client_secret
```

3. **Test MongoDB connection:**
```bash
python mongodb.mongodb_utils.py
```

## Usage

### France Travail Scraper
```bash
python scraper_francetravail.py
```

### Create Your Own Scraper
```python
from mongodb.mongodb_utils import get_collection, create_unique_index, bulk_upsert

# 1. Define your collection name
COLLECTION_NAME = "apec_raw"

# 2. Get collection
collection = get_collection(COLLECTION_NAME)
create_unique_index(collection, "id")

# 3. Scrape your data
offers = scrape_apec_data()  # Your scraping logic

# 4. Convert to list of dicts
documents = [offer.to_dict() for offer in offers]

# 5. Upsert to MongoDB
bulk_upsert(collection, documents)
```

## MongoDB Utilities Reference

### Connection Functions
- `get_mongo_client()` - Get MongoDB client
- `get_collection(name)` - Get specific collection
- `create_unique_index(collection, field)` - Create unique index

### Data Operations
- `bulk_upsert(collection, docs)` - Upsert documents (update or insert)
- `bulk_insert(collection, docs)` - Insert new documents only
- `count_documents(collection, filter)` - Count documents
- `get_latest_scraped(collection, limit)` - Get recent documents

### Collection Management
- `list_collections()` - List all collections
- `get_collection_stats(collection)` - Get collection statistics
- `drop_collection(name, confirm=True)` - Delete collection

## Best Practices

1. **Always use upsert** - Prevents duplicates
2. **Create unique index on 'id'** - Required for efficient upserts
3. **Use same DB_NAME** - All scrapers share `RUCHE_datalake`
4. **Different collections** - Each source gets its own collection
5. **Add scraped_at timestamp** - Track when data was collected

## Example: APEC Scraper Template
```python
from mongodb.mongodb_utils import get_collection, bulk_upsert
from dataclasses import dataclass, asdict

COLLECTION_NAME = "apec_raw"

@dataclass
class APECOffer:
    id: str
    title: str
    company: str
    # ... your fields

collection = get_collection(COLLECTION_NAME)
offers = scrape_apec()
docs = [offer.to_dict() for offer in offers]
bulk_upsert(collection, docs)
```
