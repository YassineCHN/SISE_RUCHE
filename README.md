# ğŸ RUCHE
Projet NLP & Text Mining â€“ Master 2 SISE (2025â€“2026) 

### Application de cherche d'emplois

<p align="center">
  <img src="streamlit/static/Logo3.png" alt="RUCHE" width="280">
</p>

![Python](https://img.shields.io/badge/Python-3.11+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![SQL](https://img.shields.io/badge/SQL-DuckDB-2E86C1?style=for-the-badge&logo=databricks&logoColor=white)
![MongoDB](https://img.shields.io/badge/MongoDB-Data%20Lake-47A248?style=for-the-badge&logo=mongodb&logoColor=white)
![MotherDuck](https://img.shields.io/badge/MotherDuck-DuckDB%20Cloud-FFD43B?style=for-the-badge&logo=duckdb&logoColor=black)
![Streamlit](https://img.shields.io/badge/Streamlit-1.30+-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)
![Plotly](https://img.shields.io/badge/Plotly-Interactive%20Viz-3F4F75?style=for-the-badge&logo=plotly&logoColor=white)
![Folium](https://img.shields.io/badge/Folium-Maps-77B829?style=for-the-badge)
![Selenium](https://img.shields.io/badge/Selenium-Web%20Scraping-43B02A?style=for-the-badge&logo=selenium&logoColor=white)
![BeautifulSoup](https://img.shields.io/badge/BeautifulSoup-HTML%20Parsing-59666C?style=for-the-badge)
![Docker](https://img.shields.io/badge/Docker-Containerization-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![Mistral](https://img.shields.io/badge/Mistral-LLM%20API-FFB703?style=for-the-badge)


## PrÃ©sentation du projet

**RUCHE** est une plateforme dâ€™analyse du marchÃ© de lâ€™emploi **Data Science & Intelligence Artificielle** en France.  
Elle combine **web scraping**, **NLP**, **machine learning**, **data warehousing** et **visualisation interactive** pour proposer :

-  **Une recherche sÃ©mantique intelligente** dâ€™offres dâ€™emploi  
-  **Une cartographie gÃ©ographique interactive** du marchÃ© de lâ€™emploi  
-  **Des analyses avancÃ©es** sur les salaires, les compÃ©tences et les tendances du marchÃ©
-  **Enregistrement de nouvelles offres** pour les utilisateurs de l'application

Le systÃ¨me repose sur une **architecture end-to-end**, depuis la collecte des donnÃ©es jusquâ€™Ã  leur exploitation analytique au sein dâ€™une application **Streamlit**.

---

## ğŸ§  Objectifs du projet

Le projet RUCHE sâ€™inscrit dans le cadre du module **NLP & Text Mining** du Master 2 SISE et rÃ©pond aux objectifs pÃ©dagogiques suivants :

- ğŸ“¥ **Constituer un corpus dâ€™offres dâ€™emploi**
  - Extraction automatisÃ©e dâ€™annonces issues de plateformes dâ€™emploi accessibles en ligne  
    (France Travail, APEC, JobTeaser, Choisir le Service Public, etc.)
  - Collecte rÃ©alisÃ©e via des techniques de **web scraping** (BeautifulSoup, Selenium) et des **API** lorsque disponibles
  - Exploitation des champs structurÃ©s lorsquâ€™ils sont disponibles  
    *(titre, missions, compÃ©tences, profil, rÃ©munÃ©ration, localisation, type de contratâ€¦)*
  - Analyse du **corps textuel complet** lorsque la structure est absente ou hÃ©tÃ©rogÃ¨ne
  - Focalisation sur les **mÃ©tiers et compÃ©tences liÃ©s Ã  la Data Science et Ã  lâ€™Intelligence Artificielle**
  - Stocker sur MongoDB (Base NoSql) dans diffÃ©rentes collections les offres scrapper
  - 6000 offres collectÃ©s 

- ğŸ—„ï¸ **Mettre en place un entrepÃ´t de donnÃ©es**
  - CrÃ©action d'une pipeline d'ETL pour **extraire** nos offre de MongoDb, les **transformer** et les **charger** dans une BDD relationnel sur MotherDuckdb
  - ModÃ©lisation sous forme de **schÃ©ma en Ã©toile** (table de faits et dimensions)
  - Stockage dans un **SGBD libre** (DuckDB via MotherDuck)
  - Connexion directe entre lâ€™application et la base de donnÃ©es analytique
  - ~4000 offres aprÃ¨s nettoyages stocker sur MotherDuck et DuckDB
    
- ğŸ§  **Appliquer des mÃ©thodes avancÃ©es de NLP et de Machine Learning**
  - Filtrage automatique des offres non pertinentes (hors data / IA)
  - Vectorisation sÃ©mantique des annonces
  - Recherche par similaritÃ© en langage naturel
  - Analyses interprÃ©tables et lisibles, y compris lors de lâ€™usage de modÃ¨les de langage (LLM)

- ğŸŒ **DÃ©velopper une application web interactive**
  - Application Python basÃ©e sur **Streamlit**
  - Interface dÃ©diÃ©e Ã  lâ€™exploration, la recherche et lâ€™analyse du corpus
  - Visualisations interactives (cartes, graphiques dynamiques, clustering)

- ğŸ—ºï¸ **IntÃ©grer une dimension gÃ©ographique**
  - Analyse territoriale Ã  lâ€™Ã©chelle des villes, dÃ©partements et rÃ©gions
  - ReprÃ©sentations cartographiques interactives

- â• **Permettre lâ€™ajout dynamique de nouvelles offres**
  - Ajout manuel ou semi-automatisÃ© dâ€™annonces (LLM - Mistral)
  - MÃ©canismes de **dÃ©tection de doublons** pour prÃ©server la qualitÃ© du corpus

- ğŸš¢ **Garantir la reproductibilitÃ© et le dÃ©ploiement**
  - DÃ©ploiement de lâ€™ensemble du systÃ¨me via une **image Docker**
  - Lâ€™utilisateur peut lancer lâ€™application sans configuration complexe

---

## ğŸ—ï¸ Architecture globale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Web Scraping â”‚ â†’  â”‚      MongoDB       â”‚ â†’  â”‚  ETL & Normalisation â”‚ â†’  â”‚        MotherDuck        â”‚ â†’  â”‚        Streamlit     â”‚
â”‚ APIs/Crawlers â”‚    â”‚ BDD NSql  (JSON)   â”‚    â”‚ Nettoyage & Enrich.  â”‚    â”‚ Data Warehouse Ã©toile    â”‚    â”‚ Recherche & Analyses â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸŒ Sources de donnÃ©es

Quatre plateformes majeures ont Ã©tÃ© exploitÃ©es :

- **France Travail**  
  API officielle, OAuth2, scraping parallÃ¨le
- **APEC**  
  Selenium + BeautifulSoup, extraction structurÃ©e offline
- **JobTeaser**  
  Anti-bot, scraping React, filtrage prÃ©coce
- **Choisir le Service Public**  
  Scraping + extraction structurÃ©e assistÃ©e par LLM (Mistral)

Les donnÃ©es brutes sont stockÃ©es en **MongoDB Atlas** (NoSQL) au format **JSON**.

---

## ğŸ—„ï¸ Data Warehouse â€“ MotherDuck

Le data warehouse repose sur **MotherDuck (DuckDB cloud)** avec :

- **SchÃ©ma en Ã©toile**
- **Table de faits** : `f_offre`
- **Dimensions** : `d_date`, `d_contrat`, `d_localisation`, `h_region`

--- 

## ğŸ¤– NLP & Machine Learning

### ğŸ” Filtrage Data / Non-Data

Approche hybride :
- rÃ¨gles expertes (regex whitelist / blacklist)
- **TF-IDF + rÃ©gression logistique**

RÃ©sultats :
- **F1-score : 0.978**
- **ROC-AUC : 0.996**
- **+67 %** dâ€™offres data rÃ©cupÃ©rÃ©es par rapport aux regex seules

---

### ğŸ” Recherche sÃ©mantique
- ModÃ¨le : `sentence-transformers/paraphrase-multilingual-mpnet-base-v2`
- RequÃªtes en **langage naturel**
- SimilaritÃ© cosinus calculÃ©e **cÃ´tÃ© base** (DuckDB)

---

## ğŸ–¥ï¸ Application Streamlit

Application **multi-pages** :
- Recherche sÃ©mantique, par mot clÃ© et filtre
- Cartographie interactive (Folium + clustering)
- Tableaux de bord analytiques (Plotly)
- Ajout manuel dâ€™offres et Chatbot LLM (Mistral) pour la structuration dâ€™offres
- Clustering sÃ©mantique (UMAP + HDBSCAN)
- Graphe de co-occurrences des compÃ©tences

ğŸ”’ Connexion sÃ©curisÃ©e Ã  MotherDuck via token  

---



## Architecture du Projet 

```
RUCHE/
â”œâ”€â”€ data/        # Bases DuckDB locales (backup et environnement de travail)
â”‚   â””â”€â”€ local.duckdb
â”‚
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile.streamlit
â”‚
â”œâ”€â”€ documentation/      # Documentation et notices techniques
â”‚   â”œâ”€â”€ notice_france_travail_scraper.md
â”‚   â”œâ”€â”€ notice_moteur_recherche_semantique.md
â”‚   â”œâ”€â”€ notice_TFIDF_ML_filtre_data_nondata.md
â”‚   â”œâ”€â”€ Rapport.md 
â”‚   â”œâ”€â”€ SISE NLP_Text Mining_Rapport_Groupe6_RUCHE.pdf
â”‚   â””â”€â”€ SISE NLP_Text Mining_Rapport_Groupe6_RUCHE.tex
â”‚
â”œâ”€â”€ dump/ # Stockage local des collections de la base NoSQL
â”‚   â”œâ”€â”€ RUCHE_datalake/
â”‚   â””â”€â”€ prelude.json
â”‚
â”œâ”€â”€ etl/        # Pipeline ETL et traitements analytiques
â”‚   â”œâ”€â”€ cleanX.py  # Fonctions de nettoyage et normalisation des donnÃ©es
â”‚   â”œâ”€â”€ config_etl.py
â”‚   â”œâ”€â”€ etl_mongo_mduck.py # Lancement principal d'ETL
â”‚   â”œâ”€â”€ etl_utils.py
â”‚   â”œâ”€â”€ etl_vectorization.py
â”‚   â”œâ”€â”€ geolocation_enrichment.py # Enrichissement gÃ©ographique (latitude / longitude via API)
â”‚   â””â”€â”€ tfidf_ml_data_filter.py  # Filtrage ML Data / Non-Data (TF-IDF + Logistic Regression)
â”‚
â”œâ”€â”€ mongodb/    # Alimentation et gestion de BDD NoSql MongoDB Ã  partir de JSON du scrapping
â”‚   â”œâ”€â”€ main_mongo.py
â”‚   â”œâ”€â”€ mongodb_load_jobteaser.py
â”‚   â”œâ”€â”€ mongodb_utils.py
â”‚   â””â”€â”€ reference_apec.py
â”‚
â”œâ”€â”€ ruche/
â”‚    â”œâ”€â”€ __init__.py
â”‚    â””â”€â”€ db.py # Fonctions de connexion centralisÃ© aux bases
â”‚
â”œâ”€â”€ scrapers/    # Scripts de collecte des offres dâ€™emploi
â”‚   â”œâ”€â”€ apec/
â”‚   â”œâ”€â”€ francetravail/
â”‚   â”œâ”€â”€ jobteaser/
â”‚   â””â”€â”€ service_public/
â”‚
â”œâ”€â”€ streamlit/   # Application web interactive Streamlit
â”‚   â”œâ”€â”€ static/   # Ressources statiques (logos, images)
â”‚   â”œâ”€â”€ .streamlit/ # contient le fichier config.toml pour le style de l'application
â”‚   â”œâ”€â”€ 1_home_page.py
â”‚   â”œâ”€â”€ 2_cartographie.py
â”‚   â”œâ”€â”€ 3_visualisation.py
â”‚   â”œâ”€â”€ 4_add_offers.py
â”‚   â”œâ”€â”€ 5_clustering.py
â”‚   â”œâ”€â”€ 6_graphe_competences.py
â”‚   â”œâ”€â”€ 7_about.py
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ config.py

â”‚
â”œâ”€â”€ .env.example # Exemple de .env 
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .python-version # UV
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ pyproject.toml #UV       
â”œâ”€â”€ README.md 
â”œâ”€â”€ requirements_streamlit.txt # Requirements allegÃ© pour Docker
â””â”€â”€ requirements.txt 

```
--- 

## âš™ï¸ Installation & Lancement (Streamlit)

---

### ğŸ“‹ PrÃ©requis

- Docker â‰¥ 20.x
- Docker Compose
- AccÃ¨s au dÃ©pÃ´t du projet (Git ou Drive)

### ğŸ“¥ RÃ©cupÃ©ration du projet

Cloner le dÃ©pÃ´t GitHub :

```bash
git clone https://github.com/RomainBuono/RUCHE.git
cd RUCHE
```

---

### ğŸ“ Arborescence requise

Pour que lâ€™application fonctionne correctement, les dossiers suivants doivent Ãªtre prÃ©sents :

```text
RUCHE/
â”œâ”€â”€ data/                     # Base DuckDB (.duckdb)
â”œâ”€â”€ streamlit/                # Application Streamlit
â”œâ”€â”€ etl/                      # Fonctions ETL utilisÃ©es dans lâ€™app
â”œâ”€â”€ documentation/            # Rapport PDF chargÃ© dans lâ€™application
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile.streamlit  # Dockerfile de lâ€™app Streamlit
â”œâ”€â”€ ruche/                    # MÃ©thodes de connexion Ã  la base
â”œâ”€â”€ requirements_streamlit.txt
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ .env                      # Variables dâ€™environnement (Ã  crÃ©er)
```
- âš ï¸ Le fichier .env doit Ãªtre placÃ© Ã  la racine du projet, au mÃªme niveau que docker-compose.yml.
- âš ï¸ Injecter les variables dâ€™environnement via un fichier `.env`

### ğŸ³ Build de lâ€™image Streamlit

Depuis la racine du projet :
```bash
docker build -f docker/Dockerfile.streamlit -t ruche-streamlit .
```

### â–¶ï¸ Lancement de lâ€™application
```bash
docker compose up
```
Lâ€™application est ensuite accessible Ã  lâ€™adresse :
-> http://localhost:8501

Pour l' arrÃªt de l'application :
```bash
docker compose down
```
## âš™ï¸ Installation alternative (sans Docker) â€” recommandÃ©e pour les performances

Cette mÃ©thode correspond Ã  une installation **locale ou avec connexions distantes**  
et constitue **lâ€™alternative la plus rapide** Ã  lâ€™exÃ©cution via Docker.

Lâ€™installation via **Docker Compose** reste pertinente pour la reproductibilitÃ©,
mais peut entraÃ®ner des temps de chargement plus Ã©levÃ©s pour lâ€™application Streamlit.

---

### ğŸ“¥ RÃ©cupÃ©ration du projet

Cloner le dÃ©pÃ´t GitHub :

```bash
git clone https://github.com/RomainBuono/RUCHE.git
cd RUCHE
```
---
### ğŸ Environnement Python

âš ï¸ Python **3.12** est requis (Python 3.13 non supportÃ©).

CrÃ©er et activer un environnement virtuel (au choix) :

**Avec `venv`**
```bash
python3.12 -m venv venv
source venv/bin/activate        # Linux / macOS
# venv\Scripts\activate         # Windows
```
**Avec `conda`**
```bash
conda create -n ruche python=3.12
conda activate ruche
```
**Avec `uv`**
```bash
uv venv --python 3.12
source .venv/bin/activate
```
---
### ğŸ“¦ Installation des dÃ©pendances
Installer les dÃ©pendances nÃ©cessaires Ã  lâ€™application Streamlit :
```bash
pip install -r requirements.txt
```
---
### ğŸ” Variables dâ€™environnement

CrÃ©er un fichier `.env` Ã  la racine du projet contenant les variables suivantes 
```bash
### ğŸ” Variables dâ€™environnement

CrÃ©er un fichier `.env` Ã  la racine du projet contenant les variables suivantes :

```env
# ---------- Connexion MongoDB distante
MONGO_URI=
MONGO_DATABASE=RUCHE_datalake
MISTRAL_API_KEY=

# ---------- Connexion DuckDB / MotherDuck
MOTHERDUCK_TOKEN=
MOTHERDUCK_DB=job_market_RUCHE   # La base doit exister dans MotherDuck (mÃªme vide)

# ---------- Connexion locale
DUCKDB_PATH=/data/local.duckdb

# ---------- Mode de connexion
# Valeurs possibles : offline (DuckDB) | online (MotherDuck)
CONNEXION_MODE=offline
```
âš ï¸ Le fichier `.env` **ne doit pas Ãªtre versionnÃ©**.

---
### ğŸ”„ Modes de connexion

Lâ€™application supporte deux modes de connexion, pilotÃ©s par la variable `CONNEXION_MODE` :

- **offline** : utilisation dâ€™une base DuckDB locale  
- **online** : connexion Ã  une base MotherDuck distante  

Le comportement de lâ€™application Streamlit sâ€™adapte automatiquement au mode sÃ©lectionnÃ©.

---
### â–¶ï¸ Lancement de lâ€™application
Lancer lâ€™application Streamlit :
```bash
cd streamlit
streamlit run app.py
```
Lâ€™application est accessible Ã  lâ€™adresse :
ğŸ‘‰ http://localhost:8501

--- 

## ğŸ“š Ressources associÃ©es

- ğŸ¥ **VidÃ©o d'installation** : **[ğŸ“º Installation Demo Video](https://youtu.be/PWfyqenFNrk)**
- ğŸ¥ **VidÃ©o de dÃ©monstration** : **[ğŸ“º Demonstration Video](https://youtu.be/gIqiBeyKKjI)**
- ğŸ“„ **Rapport acadÃ©mique (PDF)**  : [Projet NLP & Text Mining â€“ Rapport RUCHE (Groupe 6)](/documentation/SISE%20NLP_Text%20Mining_Rapport_Groupe6_RUCHE.pdf)
- ğŸ“˜ **Notice technique â€“ Filtrage ML Data / Non-Data**  : [TF-IDF & RÃ©gression logistique](documentation/notice_TFIDF_ML_filtre_data_nondata.md)
- ğŸ“˜ **Notice technique â€“ Scraper France Travail**  : [API & Web Scraping France Travail](documentation/notice_france_travail_scraper.md)
- ğŸ“˜ **Notice technique â€“ Moteur de recherche sÃ©mantique**  : [Recherche vectorielle & similaritÃ© cosinus](documentation/notice_moteur_recherche_semantique.md)
- ğŸ“š **API-RÃ©fÃ©rentiel gÃ©ographique franÃ§ais** : [API GÃ©olocalisation](https://data.enseignementsup-recherche.gouv.fr/explore/dataset/fr-esr-referentiel-geographique/api/)




## ğŸ‘¥ Ã‰quipe

- Romain Buono
-  Yassine Cheniour
- MilÃ©na Gordien-Piquet
- Anne-Camille Vial

#### ğŸ“ Master 2 SISE â€“ UniversitÃ© Lyon 2
#### ğŸ‘¨â€ğŸ« Encadrant : M. Ricco Rakotomalala

---
