# ğŸ RUCHE
Projet NLP & Text Mining â€“ Master 2 SISE (2025â€“2026) 

### Application de cherche d'emplois

![Logo](streamlit/static/Logo3.png)

## PrÃ©sentation du projet

**RUCHE** est une plateforme dâ€™analyse du marchÃ© de lâ€™emploi **Data Science & Intelligence Artificielle** en France.  
Elle combine **web scraping**, **NLP**, **machine learning**, **data warehousing** et **visualisation interactive** pour proposer :

-  **Une recherche sÃ©mantique intelligente** dâ€™offres dâ€™emploi  
-  **Une cartographie gÃ©ographique interactive** du marchÃ© de lâ€™emploi  
-  **Des analyses avancÃ©es** sur les salaires, les compÃ©tences et les tendances du marchÃ©
-  **Enregistrement de nouvelles offres** pour les utilisateurs de l'application

Le systÃ¨me repose sur une **architecture end-to-end**, depuis la collecte des donnÃ©es jusquâ€™Ã  leur exploitation analytique au sein dâ€™une application **Streamlit**.

---

## ğŸ§  Objectifs du projet

Le projet RUCHE sâ€™inscrit dans le cadre du module **NLP & Text Mining** du Master 2 SISE et rÃ©pond aux objectifs pÃ©dagogiques suivants :

- ğŸ“¥ **Constituer un corpus dâ€™offres dâ€™emploi**
  - Extraction automatisÃ©e dâ€™annonces issues de plateformes dâ€™emploi accessibles en ligne  
    (France Travail, APEC, JobTeaser, Choisir le Service Public, etc.)
  - Collecte rÃ©alisÃ©e via des techniques de **web scraping** (BeautifulSoup, Selenium) et des **API** lorsque disponibles

- ğŸ§¾ **Analyser les annonces dans leurs diffÃ©rentes dimensionnalitÃ©s**
  - Exploitation des champs structurÃ©s lorsquâ€™ils sont disponibles  
    *(titre, missions, compÃ©tences, profil, rÃ©munÃ©ration, localisation, type de contratâ€¦)*
  - Analyse du **corps textuel complet** lorsque la structure est absente ou hÃ©tÃ©rogÃ¨ne
  - Focalisation sur les **mÃ©tiers et compÃ©tences liÃ©s Ã  la Data Science et Ã  lâ€™Intelligence Artificielle**
  - Stocker sur MongoDB (Base NoSql) dans diffÃ©rentes collections les offres scrapper

- ğŸ—„ï¸ **Mettre en place un entrepÃ´t de donnÃ©es**
  - CrÃ©action d'une pipeline d'ETL pour **extraire** nos offre de MongoDb, les **transformer** et les **charger** dans une BDD relationnel sur MotherDuckdb
  - ModÃ©lisation sous forme de **schÃ©ma en Ã©toile** (table de faits et dimensions)
  - Stockage dans un **SGBD libre** (DuckDB via MotherDuck)
  - Connexion directe entre lâ€™application et la base de donnÃ©es analytique
    
- ğŸ§  **Appliquer des mÃ©thodes avancÃ©es de NLP et de Machine Learning**
  - Filtrage automatique des offres non pertinentes (hors data / IA)
  - Vectorisation sÃ©mantique des annonces
  - Recherche par similaritÃ© en langage naturel
  - Analyses interprÃ©tables et lisibles, y compris lors de lâ€™usage de modÃ¨les de langage (LLM)

- ğŸŒ **DÃ©velopper une application web interactive**
  - Application Python basÃ©e sur **Streamlit**
  - Interface dÃ©diÃ©e Ã  lâ€™exploration, la recherche et lâ€™analyse du corpus
  - Visualisations interactives (cartes, graphiques dynamiques, clustering)

- ğŸ—ºï¸ **IntÃ©grer une dimension gÃ©ographique**
  - Analyse territoriale Ã  lâ€™Ã©chelle des villes, dÃ©partements et rÃ©gions
  - ReprÃ©sentations cartographiques interactives

- â• **Permettre lâ€™ajout dynamique de nouvelles offres**
  - Ajout manuel ou semi-automatisÃ© dâ€™annonces (LLM - Mistral)
  - MÃ©canismes de **dÃ©tection de doublons** pour prÃ©server la qualitÃ© du corpus

- ğŸš¢ **Garantir la reproductibilitÃ© et le dÃ©ploiement**
  - DÃ©ploiement de lâ€™ensemble du systÃ¨me via une **image Docker**
  - Lâ€™utilisateur peut lancer lâ€™application sans configuration complexe

---

## ğŸ—ï¸ Architecture globale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Web Scraping â”‚ â†’  â”‚      MongoDB       â”‚ â†’  â”‚  ETL & Normalisation â”‚ â†’  â”‚        MotherDuck        â”‚ â†’  â”‚        Streamlit      â”‚
â”‚ APIs/Crawlers â”‚    â”‚ Data Lake (JSON)   â”‚    â”‚ Nettoyage & Enrich.  â”‚    â”‚ Data Warehouse Ã©toile   â”‚    â”‚ Recherche & Analyses  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸŒ Sources de donnÃ©es

Quatre plateformes majeures ont Ã©tÃ© exploitÃ©es :

- **France Travail**  
  API officielle, OAuth2, scraping parallÃ¨le
- **APEC**  
  Selenium + BeautifulSoup, extraction structurÃ©e offline
- **JobTeaser**  
  Anti-bot, scraping React, filtrage prÃ©coce
- **Choisir le Service Public**  
  Scraping + extraction structurÃ©e assistÃ©e par LLM (Mistral)

ğŸ‘‰ Les donnÃ©es brutes sont stockÃ©es en **MongoDB Atlas** (NoSQL) au format **JSON**.

---

## ğŸ—„ï¸ Data Warehouse â€“ MotherDuck

Le data warehouse repose sur **MotherDuck (DuckDB cloud)** avec :

- **SchÃ©ma en Ã©toile**
- **Table de faits** : `f_offre`
- **Dimensions** : `d_date`, `d_contrat`, `d_localisation`, `h_region`

--- 

## Architecture du Projet 

```
RUCHE/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ backup_job_market.duckdb
â”‚   â””â”€â”€ local.duckdb
â”‚
â”œâ”€â”€ scraping/
â”‚   â”œâ”€â”€ francetravail/
â”‚   â”œâ”€â”€ apec/
â”‚   â”œâ”€â”€ jobteaser/
â”‚   â””â”€â”€ service_public/
â”‚
â”œâ”€â”€ mongodb/
â”‚   â”œâ”€â”€ main_mongo.py
â”‚   â”œâ”€â”€ reference_apec.py
â”‚   â”œâ”€â”€ mongodb_load_jobteaser.py
â”‚   â””â”€â”€ mongodb_utils.py
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ cleanX.py #Tout les "clean" fpnction de nettoyage de donnÃ©e
â”‚   â”œâ”€â”€ config_etl.py
â”‚   â”œâ”€â”€ etl_utils.py
â”‚   â”œâ”€â”€ etl_vectorization.py
â”‚   â”œâ”€â”€ tfidf_ml_data_filter.py
â”‚   â”œâ”€â”€ geolocation_enrichment.py # API pour longÃ©tude et latitude 
â”‚   â””â”€â”€ etl_motherduck.py
â”‚
â”œâ”€â”€ streamlit_app/
â”‚   â”œâ”€â”€ 1_home_page.py
â”‚   â”œâ”€â”€ 2_cartographie.py
â”‚   â”œâ”€â”€ 3_visualisation.py
â”‚   â”œâ”€â”€ 4_add_offers.py
â”‚   â”œâ”€â”€ 5_clustering.py
â”‚   â”œâ”€â”€ 6_graphe_competences.py
â”‚   â”œâ”€â”€ 7_llm.py
â”‚   â”œâ”€â”€ 8_about.py
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ static/ # Logo & images
â”‚   â”œâ”€â”€ db/
â”‚   â””â”€â”€ analyse_competences/
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ Rapport.md
â”‚   â”œâ”€â”€ notice_france_travail_scraper.md
â”‚   â”œâ”€â”€ notice_TFIDF_ML_filtre_data_nondata.md
â”‚   â””â”€â”€ notice_moteur_recherche_semantique.md
â”‚
â”œâ”€â”€ duck_to_mother.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ test_connexion_duckdb.py
â”œâ”€â”€ test_creation_duckdb.py
â””â”€â”€ README.md

```
--- 

## ğŸš€ Lancer lâ€™application

1. **Installer les dÃ©pendances:**
```bash
pip install -r requirements_mongodo_ftscraper.txt
```

2. **Configurer `.env` file:**
```env
MOTHERDUCK_TOKEN=MOTHERDUCKDB_KEY
MOTHERDUCK_DATABASE = "job_market_RUCHE"
```

3. **Lancer Streamlit**
```bash
streamlit run app.py
```

--- 
## ğŸ‘¥ Ã‰quipe

- Romain Buono
-  Yassine Cheniour
- MilÃ©na Gordien-Piquet
- Anne-Camille Vial

#### ğŸ“ Master 2 SISE â€“ UniversitÃ© Lyon 2####
#### ğŸ‘¨â€ğŸ« Encadrant : M. Ricco Rakotomalala####

---
